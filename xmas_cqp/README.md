# XMAS-CQP

**XMAS-CQP (eXplainable Model-Aware Software Code Quality Prediction)**  
is a model-centric, reproducible explanation pipeline for software defect / code quality prediction.

The system strictly separates **decision construction** and **decision explanation**, enabling
faithful, schema-constrained, and repeatable explanations for empirical evaluation.

---

## ğŸ“ Project Structure

```

xmas_cqp/
â”œâ”€â”€ agents/                # Preprocessor & Explainer agents
â”œâ”€â”€ cli.py                 # CLI entry point
â”œâ”€â”€ config/
â”‚   â””â”€â”€ explainer.yaml     # Main experiment configuration
â”œâ”€â”€ prompts/               # System & user prompts
â”œâ”€â”€ schemas/               # JSON schema for explanations
â”œâ”€â”€ llm/                   # LLM client & utilities
â””â”€â”€ README.md

````

---

## ğŸ“¥ Input Format

XMAS-CQP expects **JSONL input (NOT CSV)**.

Each line represents one model prediction task:

```json
{
  "task": "explain_model_prediction",
  "input": {
    "features": { "...": "model-visible features" },
    "model_output": {
      "prediction": "clean | buggy",
      "probability": 0.0
    }
  },
  "metadata": {
    "dataset": "openstack",
    "sample_id": 0,
    "commit_id": "..."
  }
}
````

---

## ğŸ“¤ Output Structure

Each repeated run is written to an isolated directory:

```
results/{dataset}/{project_version}/run_{run_id}/
â”œâ”€â”€ processed.jsonl        # Deterministic decision IR
â”œâ”€â”€ explanations.jsonl     # Schema-constrained explanations
â””â”€â”€ errors.jsonl           # Failure records (if any)
```

---

## â–¶ï¸ Running Experiments (Windows / PowerShell)

All commands below are written for **Windows PowerShell**.

General command format:

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset <DATASET_NAME> `
  --input <INPUT_JSONL> `
  --run_id <RUN_ID>
```

---

## ğŸ” Repeated Runs (N = 5)

The following commands perform **five repeated runs** on the same dataset and input,
used for **stability, robustness, and variance analysis**.

---

### â–¶ï¸ Run 1

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset openstack `
  --input data/rq1_samples/openstack_rq1_input.jsonl `
  --run_id 1
```

---

### â–¶ï¸ Run 2

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset openstack `
  --input data/rq1_samples/openstack_rq1_input.jsonl `
  --run_id 2
```

---

### â–¶ï¸ Run 3

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset openstack `
  --input data/rq1_samples/openstack_rq1_input.jsonl `
  --run_id 3
```

---

### â–¶ï¸ Run 4

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset openstack `
  --input data/rq1_samples/openstack_rq1_input.jsonl `
  --run_id 4
```

---

### â–¶ï¸ Run 5

```powershell
python -m xmas_cqp.cli run `
  --config xmas_cqp/config/explainer.yaml `
  --dataset openstack `
  --input data/rq1_samples/openstack_rq1_input.jsonl `
  --run_id 5
```

---

## ğŸ”¬ Reproducibility Notes

* All preprocessing is **deterministic**
* Each `run_id` produces **independent outputs**
* Explanation randomness (LLM) is evaluated via repeated runs
* All failures are explicitly logged in `errors.jsonl`

This setup supports empirical analysis of:

* Explanation stability
* Feature attribution consistency
* Hallucination rate
* Run-to-run variance

---

## âš ï¸ Common Pitfalls

* âŒ Do NOT use CSV as input
* âŒ Do NOT reuse the same `run_id`
* âœ… Always use JSONL input
* âœ… Increment `run_id` for each repetition

---

## ğŸ“– Intended Use

XMAS-CQP is designed for:

* Explainable Software Defect Prediction (SDP)
* Faithfulness and stability evaluation of XAI methods
* RQ-driven empirical software engineering research
* Reproducible academic experimentation

---

## ğŸ“œ License

For research and academic use only.
